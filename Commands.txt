to get datasets from Kaggle :-
    https://www.kaggle.com/c/data-science-bowl-2017/discussion/27698
    7za e stage1.7z

to build a jar :-
    mvn package -DskipTests
    gzip -k dl4j_test_scripts-1.0-SNAPSHOT-bin.jar
    cd /home/sudheera/Documents/MSc/final_project/s3_upload_jar
    mv /home/sudheera/Documents/MSc/final_project/dl4j_test_scripts/target/dl4j_test_scripts-1.0-SNAPSHOT-bin.jar.gz .

Upload the jar into s3 bucket :-
    from here referred :- https://www.jtouzi.net/uploading-a-large-file-to-amazon-web-services-s3/

    for i in {1..15}; do dd if=dl4j_test_scripts-1.0-SNAPSHOT-bin.jar.gz of=dl4j_test_scripts-1.0-SNAPSHOT-bin.jar"$i".gz \
    bs=1024k skip=$[i*50 - 50] count=50; done

    rm dl4j_test_scripts-1.0-SNAPSHOT-bin.jar.gz

    aws s3api create-multipart-upload --bucket dl4j --key dl4j_test_scripts-1.0-SNAPSHOT-bin.jar.gz
        <--Note UploadId from the json output = "MyLongUploadId"-->

    for i in {1..15}; do aws s3api upload-part --bucket dl4j --key dl4j_test_scripts-1.0-SNAPSHOT-bin.jar.gz \
    --upload-id <--MyLongUploadId--> \
    --part-number $[i] --body dl4j_test_scripts-1.0-SNAPSHOT-bin.jar"$i".gz; done
            <--Copy ETag values of each partition into a text file-->

    Create a JSON file MyMultiPartUpload.json containing the following:
        {
        "Parts": [
        ...
        {
        "ETag": "\"ETagValue-k\"",
        "PartNumber": k
        },
        ...
        ]
        }

    aws s3api complete-multipart-upload --bucket dl4j \
    --key dl4j_test_scripts-1.0-SNAPSHOT-bin.jar.gz \
    --upload-id MyLongUploadId \
    --multipart-upload file://MyMultiPartUpload.json

    check upload - aws s3 ls s3://dl4j/

Cresting a EMR spark cluster :-
    https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-spark-launch.html

    cheapest EC2 - m4.large

    video :- https://www.youtube.com/watch?v=hSWkKk36TS8&ab_channel=LevelUp (6:20 onwards)
    create EMR cluster
    adding ssh inbound rule

spark submit command :-

    ssh -i iit-lab-key-pair.pem <--hadoop@ec2-3-139-102-224.us-east-2.compute.amazonaws.com-->
    aws s3 cp s3://dl4j/dl4j_test_scripts-1.0-SNAPSHOT-bin.jar.gz .
    gzip -d dl4j_test_scripts-1.0-SNAPSHOT-bin.jar.gz

    <-- Running pre-processor Jar :-

    mkdir Cifar10
    java -cp dl4j_test_scripts-1.0-SNAPSHOT-bin.jar cifar10.PreprocessLocal --localSaveDir /home/hadoop/Cifar10
    aws s3 cp --recursive /home/hadoop/Cifar10 s3://dl4jcifar10/ -> put Cifar10 data into S3

spark-submit --class cifar10.TrainSparkCifar10 --master yarn --num-executors 2 --executor-cores 2 \
--executor-memory 1g --conf spark.driver.memoryOverhead=1g --conf spark.executor.memoryOverhead=1g \
--deploy-mode cluster dl4j_test_scripts-1.0-SNAPSHOT-bin.jar --avgFreq 10 \
--dataPath s3://dl4jcifar10/

Clean up :-
    delete S3 bucket
    delete EMR cluster